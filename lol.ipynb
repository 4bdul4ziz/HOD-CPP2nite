{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "*Data Science- Round 2 (Final) Details*\n",
    "\n",
    "Link:  https://www.kaggle.com/competitions/im-somewhat-of-a-cybersecurity-analyst-myself\n",
    "\n",
    "1. The training data will be provided in a `CSV` format in the `training` directory\n",
    "2. Youâ€™ll find 50 folds of the data in the `training/uploads/` directory.\n",
    "    1. Here, you are expected to build the data loader using `pd.concat` and looping.\n",
    "3. Once, you are done with this, you will be expected to build multiple models and train it using the training data generated from the previous step.\n",
    "4. Next, you will be expected to get the predictions of the test data, in the expected format. The submission file contains:\n",
    "    1. `machine_id`: An identifier to identify the machines\n",
    "    2. `malware_status`: The output of the model (which is expected to be either 0 or 1).\n",
    "5. Then just submit the above solution to Kaggle by converting the above to a csv format.\n",
    "6. You will be ranked on a Public Leaderboard which is not a definite result as it will be tested on a portion of the test data.\n",
    "7. The final ranks will be made available after the contests ends as you will expected to make you kernels/`.ipynb` files public on Kaggle.\n",
    "    1. If this is not done, you WILL be disqualified even if you get the highest accuracy that your peers.\n",
    "8. It is highly recommended to use the Kaggle kernels to build the models as you are free to use GPUs as well as TPUs as per your needs.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "final submission csv file has two columns, machine_id and malware_status\n",
    "with 3921483 rows and a header\n",
    "this file should be generated after the model is trained and tested\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "train1 = pd.read_parquet(\n",
    "    '/Users/abdul/Desktop/Programming/HOD-CPP2nite/data/training_data/train1.parquet')\n",
    "train2 = pd.read_parquet(\n",
    "    '/Users/abdul/Desktop/Programming/HOD-CPP2nite/data/training_data/train2.parquet')\n",
    "train3 = pd.read_parquet(\n",
    "    '/Users/abdul/Desktop/Programming/HOD-CPP2nite/data/training_data/train3.parquet')\n",
    "train = pd.concat([train1, train2, train3], axis=0)\n",
    "\n",
    "test1 = pd.read_parquet(\n",
    "    '/Users/abdul/Desktop/Programming/HOD-CPP2nite/data/testing_data/test1.parquet')\n",
    "test2 = pd.read_parquet(\n",
    "    '/Users/abdul/Desktop/Programming/HOD-CPP2nite/data/testing_data/test2.parquet')\n",
    "test3 = pd.read_parquet(\n",
    "    '/Users/abdul/Desktop/Programming/HOD-CPP2nite/data/testing_data/test3.parquet')\n",
    "test = pd.concat([test1, test2, test3], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5950285, 83)\n",
      "(3971198, 82)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing columns with 50% or more null values\n",
    "train = train.dropna(thresh=len(train)*0.5, axis=1)\n",
    "test = test.dropna(thresh=len(test)*0.5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5950285, 76)\n",
      "(3971198, 75)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['security_product_name', 'machine_version', 'app_version',\n",
      "       'av_sig_version', 'is_beta', 'rtp_state', 'is_sxs_passive', 'av_status',\n",
      "       'av_prod_installed', 'av_prod_enabled', 'has_tpm', 'country_id',\n",
      "       'city_id', 'org_id', 'geo_id', 'local_name_id', 'platform', 'processor',\n",
      "       'os_version', 'os_build', 'os_suite', 'os_ptfm_sub_release',\n",
      "       'os_build_lab', 'sku_edition', 'is_protected', 'auto_sample_opt_in',\n",
      "       's_mode', 'ie_ver_id', 'smart_screen', 'firewall', 'uac_luaenable',\n",
      "       'mdc2_form_factor', 'device_family', 'name_id', 'oem_model_id',\n",
      "       'core_count_processer', 'manufacturer_id', 'model_id', 'disk_capacity',\n",
      "       'disk_type', 'system_volume_capacity', 'has_optical_drive',\n",
      "       'ram_capacity', 'chassis_type', 'diagonal_display_size',\n",
      "       'display_resolution_horizontal', 'display_resolution_vertical',\n",
      "       'power_profile', 'number_of_charges_for_battery', 'version',\n",
      "       'architecture', 'branch', 'build_number', 'build_revision', 'edition',\n",
      "       'sku', 'install_type_name', 'install_language_identifier',\n",
      "       'ui_locale_identifier', 'auto_update_opt', 'os_type_status',\n",
      "       'state_name', 'activation_channel', 'flights_activity', 'flight_ring',\n",
      "       'firmware_manufacturer_id', 'firmware_version_id', 'secure_boot_status',\n",
      "       'virtual_dev_status', 'touch_support', 'pen_support', 'aoac_support',\n",
      "       'is_gamer', 'region_id', 'malware_status', 'machine_id'],\n",
      "      dtype='object')\n",
      "Index(['security_product_name', 'machine_version', 'app_version',\n",
      "       'av_sig_version', 'is_beta', 'rtp_state', 'is_sxs_passive', 'av_status',\n",
      "       'av_prod_installed', 'av_prod_enabled', 'has_tpm', 'country_id',\n",
      "       'city_id', 'org_id', 'geo_id', 'local_name_id', 'platform', 'processor',\n",
      "       'os_version', 'os_build', 'os_suite', 'os_ptfm_sub_release',\n",
      "       'os_build_lab', 'sku_edition', 'is_protected', 'auto_sample_opt_in',\n",
      "       's_mode', 'ie_ver_id', 'smart_screen', 'firewall', 'uac_luaenable',\n",
      "       'mdc2_form_factor', 'device_family', 'name_id', 'oem_model_id',\n",
      "       'core_count_processer', 'manufacturer_id', 'model_id', 'disk_capacity',\n",
      "       'disk_type', 'system_volume_capacity', 'has_optical_drive',\n",
      "       'ram_capacity', 'chassis_type', 'diagonal_display_size',\n",
      "       'display_resolution_horizontal', 'display_resolution_vertical',\n",
      "       'power_profile', 'number_of_charges_for_battery', 'version',\n",
      "       'architecture', 'branch', 'build_number', 'build_revision', 'edition',\n",
      "       'sku', 'install_type_name', 'install_language_identifier',\n",
      "       'ui_locale_identifier', 'auto_update_opt', 'os_type_status',\n",
      "       'state_name', 'activation_channel', 'flights_activity', 'flight_ring',\n",
      "       'firmware_manufacturer_id', 'firmware_version_id', 'secure_boot_status',\n",
      "       'virtual_dev_status', 'touch_support', 'pen_support', 'aoac_support',\n",
      "       'is_gamer', 'region_id', 'machine_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing na values with hot deck imputation\n",
    "train = train.fillna(method='ffill')\n",
    "test = test.fillna(method='ffill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "security_product_name    0\n",
      "machine_version          0\n",
      "app_version              0\n",
      "av_sig_version           0\n",
      "is_beta                  0\n",
      "                        ..\n",
      "aoac_support             0\n",
      "is_gamer                 0\n",
      "region_id                0\n",
      "malware_status           0\n",
      "machine_id               0\n",
      "Length: 76, dtype: int64\n",
      "security_product_name    0\n",
      "machine_version          0\n",
      "app_version              0\n",
      "av_sig_version           0\n",
      "is_beta                  0\n",
      "                        ..\n",
      "pen_support              0\n",
      "aoac_support             0\n",
      "is_gamer                 0\n",
      "region_id                0\n",
      "machine_id               0\n",
      "Length: 75, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for null values\n",
    "print(train.isnull().sum())\n",
    "print(test.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicate values\n",
    "print(train.duplicated().sum())\n",
    "print(test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "#model evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#feature selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "#feature extraction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "#feature engineering\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X = train.iloc[:, 1:-1].values\n",
    "y = train.iloc[:, -1].values\n",
    "X_test = test.iloc[:, 1:].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encoding categorical data\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "\"\"\" sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_val = sc.transform(X_val)\n",
    "X_test = sc.transform(X_test)\n",
    " \"\"\"\n",
    "# model build \n",
    "for model in [RandomForestClassifier(), LogisticRegression(), SVC(), DecisionTreeClassifier(), KNeighborsClassifier(), GaussianNB(), GradientBoostingClassifier(), AdaBoostClassifier(), ExtraTreesClassifier(), BaggingClassifier()]:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(model)\n",
    "    print(accuracy_score(y_val, y_pred))\n",
    "    print(average_precision_score(y_val, y_pred))\n",
    "    print('')\n",
    "    pickle.dump(model, open('model{}.pkl'.format(model), 'wb'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
